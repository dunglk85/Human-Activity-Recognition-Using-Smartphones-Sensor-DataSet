{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><p style=\"color: green\">Conclusion</p></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ********** Machine Learning Model Comparision ************\n",
      "+---------------------+------------------------+----------+\n",
      "|      Model Name     | Hyperparameter Tunning | Accuracy |\n",
      "+---------------------+------------------------+----------+\n",
      "| Logistic Regression |          Done          |  95.83%  |\n",
      "|     Linear SVC      |          Done          |  96.47%  |\n",
      "|  rbf SVM classifier |          Done          |  96.27%  |\n",
      "|     DecisionTree    |          Done          |  86.46%  |\n",
      "|    Random Forest    |          Done          |  92.6%   |\n",
      "+---------------------+------------------------+----------+\n",
      "\n",
      "\n",
      " ********************************* Deep Learning LSTM Model Comparision ***********************************\n",
      "+-------------------------------------------+------------------------+--------------------------+----------+\n",
      "|                 Model Name                | Hyperparameter Tunning | categorical_crossentropy | Accuracy |\n",
      "+-------------------------------------------+------------------------+--------------------------+----------+\n",
      "|       LSTM With 1_Layer(neurons:32)       |          Done          |           0.47           |   0.90   |\n",
      "| LSTM With 2_Layer(neurons:48, neurons:32) |          Done          |           0.39           |   0.90   |\n",
      "| LSTM With 2_Layer(neurons:64, neurons:48) |          Done          |           0.27           |   0.91   |\n",
      "+-------------------------------------------+------------------------+--------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "ptable1 = PrettyTable()\n",
    "ptable1.title = \" Model Comparision \"\n",
    "ptable1.field_names = ['Model Name','Hyperparameter Tunning', 'Accuracy']\n",
    "\n",
    "print(\"\\n\\n ********** Machine Learning Model Comparision ************\")\n",
    "ptable1.add_row([\"Logistic Regression\",\"Done\",\"95.83%\"])\n",
    "ptable1.add_row([\"Linear SVC  \",\"Done\",\"96.47%\"])\n",
    "ptable1.add_row([\"rbf SVM classifier\",\"Done\",\"96.27%\"])\n",
    "ptable1.add_row([\"DecisionTree\",\"Done\",\"86.46%\"])\n",
    "ptable1.add_row([\"Random Forest\",\"Done\",\"92.6%\"])\n",
    "\n",
    "print(ptable1)\n",
    "# *****************************************************************\n",
    "\n",
    "ptable2 = PrettyTable()\n",
    "ptable2.title = \" Model Comparision \"\n",
    "ptable2.field_names = ['Model Name','Hyperparameter Tunning', 'categorical_crossentropy', 'Accuracy']\n",
    "\n",
    "print(\"\\n\\n ********************************* Deep Learning LSTM Model Comparision ***********************************\")\n",
    "ptable2.add_row([\"LSTM With 1_Layer(neurons:32)\",\"Done\",\"0.47\", \"0.90\"])\n",
    "ptable2.add_row([\"LSTM With 2_Layer(neurons:48, neurons:32)\",\"Done\",\"0.39\", \"0.90\"])\n",
    "ptable2.add_row([\"LSTM With 2_Layer(neurons:64, neurons:48)\",\"Done\",\"0.27\", \"0.91\"])\n",
    "\n",
    "print(ptable2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ___Machine Learning Model Comparision___ :  We can choose ___Linear SVC___ or ___rbf SVM classifier___ or ___Logistic Regression___ as our best model while applying ML Classical Model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ___Deep Learning LSTM Model Comparision___ : When we talking about LSTM Model, here with LSTM we are using simple RAW data(in ML model we are using Single engineered data made by an expert), but we can see the result without any FE data, LSTM perform very-very well and got highest 91% accuracy with 2_layer LSTM with hyperparameter Tunning and also we can clearly when we are increasing LSTM layer and Hyperparameter Tunning the cross-entropy value is decreasing and Accuracy is increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def baum_welch(observed_sequence, num_states, num_observations, num_iterations):\n",
    "    # Initialization\n",
    "    transition_matrix = np.random.rand(num_states, num_states)\n",
    "    transition_matrix /= np.sum(transition_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    emission_matrix = np.random.rand(num_states, num_observations)\n",
    "    emission_matrix /= np.sum(emission_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    initial_state_probs = np.random.rand(num_states)\n",
    "    initial_state_probs /= np.sum(initial_state_probs)\n",
    "    \n",
    "    # Iterate through the Baum-Welch algorithm\n",
    "    for iteration in range(num_iterations):\n",
    "        # Forward pass\n",
    "        forward_probs = forward(observed_sequence, transition_matrix, emission_matrix, initial_state_probs)\n",
    "        \n",
    "        # Backward pass\n",
    "        backward_probs = backward(observed_sequence, transition_matrix, emission_matrix)\n",
    "        \n",
    "        # Expectation step\n",
    "        expected_state_counts, expected_transition_counts, expected_observation_counts = \\\n",
    "            expectation(observed_sequence, forward_probs, backward_probs, transition_matrix, emission_matrix)\n",
    "        \n",
    "        # Maximization step\n",
    "        transition_matrix, emission_matrix, initial_state_probs = \\\n",
    "            maximization(expected_state_counts, expected_transition_counts, expected_observation_counts)\n",
    "    \n",
    "    return transition_matrix, emission_matrix, initial_state_probs\n",
    "\n",
    "def forward(observed_sequence, transition_matrix, emission_matrix, initial_state_probs):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_observations = len(observed_sequence)\n",
    "    \n",
    "    forward_probs = np.zeros((num_states, num_observations))\n",
    "    forward_probs[:, 0] = initial_state_probs * emission_matrix[:, observed_sequence[0]]\n",
    "    \n",
    "    for t in range(1, num_observations):\n",
    "        for j in range(num_states):\n",
    "            forward_probs[j, t] = np.sum(forward_probs[:, t-1] * transition_matrix[:, j]) * emission_matrix[j, observed_sequence[t]]\n",
    "    \n",
    "    return forward_probs\n",
    "\n",
    "def backward(observed_sequence, transition_matrix, emission_matrix):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_observations = len(observed_sequence)\n",
    "    \n",
    "    backward_probs = np.zeros((num_states, num_observations))\n",
    "    backward_probs[:, -1] = 1.0\n",
    "    \n",
    "    for t in range(num_observations-2, -1, -1):\n",
    "        for i in range(num_states):\n",
    "            backward_probs[i, t] = np.sum(transition_matrix[i, :] * emission_matrix[:, observed_sequence[t+1]] * backward_probs[:, t+1])\n",
    "    \n",
    "    return backward_probs\n",
    "\n",
    "def expectation(observed_sequence, forward_probs, backward_probs, transition_matrix, emission_matrix):\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    num_observations = len(observed_sequence)\n",
    "    \n",
    "    expected_state_counts = forward_probs * backward_probs\n",
    "    expected_state_counts /= np.sum(expected_state_counts, axis=0, keepdims=True)\n",
    "    \n",
    "    expected_transition_counts = np.zeros((num_states, num_states, num_observations-1))\n",
    "    expected_observation_counts = np.zeros((num_states, num_observations))\n",
    "    \n",
    "    for t in range(num_observations-1):\n",
    "        expected_transition_counts[:, :, t] = \\\n",
    "            (forward_probs[:, t][:, np.newaxis] * transition_matrix * emission_matrix[:, observed_sequence[t+1]] * backward_probs[:, t+1]).T\n",
    "        expected_transition_counts[:, :, t] /= np.sum(expected_transition_counts[:, :, t])\n",
    "        \n",
    "        expected_observation_counts[:, t] = expected_state_counts[:, t] * (emission_matrix[:, observed_sequence[t]] / np.sum(emission_matrix[:, observed_sequence[t]]))\n",
    "    \n",
    "def maximization(expected_state_counts, expected_transition_counts, expected_observation_counts):\n",
    "    num_states = expected_state_counts.shape[0]\n",
    "    num_observations = expected_observation_counts.shape[1]\n",
    "    \n",
    "    # Update transition matrix\n",
    "    updated_transition_matrix = np.sum(expected_transition_counts, axis=2)\n",
    "    updated_transition_matrix /= np.sum(updated_transition_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    # Update emission matrix\n",
    "    updated_emission_matrix = np.zeros_like(expected_observation_counts)\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        for k in range(num_observations):\n",
    "            updated_emission_matrix[i, k] = np.sum(expected_observation_counts[i, :][observed_sequence == k])\n",
    "    \n",
    "    updated_emission_matrix /= np.sum(updated_emission_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    # Update initial state probabilities\n",
    "    updated_initial_state_probs = expected_state_counts[:, 0]\n",
    "    updated_initial_state_probs /= np.sum(updated_initial_state_probs)\n",
    "    \n",
    "    return updated_transition_matrix, updated_emission_matrix, updated_initial_state_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#generating initial probabilities\n",
    "\n",
    "#transition probabilities\n",
    "transition = np.array([[0.8,0.1],\n",
    "                       [0.1,0.8]])\n",
    "#Emission probabilities\n",
    "emission = np.array([[0.1,0.2,0.7],\n",
    "                     [0.7,0.2,0.1]])\n",
    "\n",
    "#defining states and sequence symbols\n",
    "states = ['H','C']\n",
    "states_dic = {'H':0, 'C':1}\n",
    "sequence_syms = {'1':0,'2':1,'3':2}\n",
    "sequence = ['1','2','3']\n",
    "\n",
    "#test sequence\n",
    "test_sequence = '331122313'\n",
    "test_sequence = [x for x in test_sequence]\n",
    "\n",
    "#probabilities of going to end state\n",
    "end_probs = [0.1, 0.1]\n",
    "#probabilities of going from start state\n",
    "start_probs = [0.5, 0.5]\n",
    "\n",
    "\n",
    "#function to find forward probabilities\n",
    "def forward_probs():\n",
    "    # node values stored during forward algorithm\n",
    "    node_values_fwd = np.zeros((len(states), len(test_sequence)))\n",
    "\n",
    "    for i, sequence_val in enumerate(test_sequence):\n",
    "        for j in range(len(states)):\n",
    "            # if first sequence value then do this\n",
    "            if (i == 0):\n",
    "                node_values_fwd[j, i] = start_probs[j] * emission[j, sequence_syms[sequence_val]]\n",
    "            # else perform this\n",
    "            else:\n",
    "                values = [node_values_fwd[k, i - 1] * emission[j, sequence_syms[sequence_val]] * transition[k, j] for k in\n",
    "                          range(len(states))]\n",
    "                node_values_fwd[j, i] = sum(values)\n",
    "\n",
    "    #end state value\n",
    "    end_state = np.multiply(node_values_fwd[:, -1], end_probs)\n",
    "    end_state_val = sum(end_state)\n",
    "    return node_values_fwd, end_state_val\n",
    "\n",
    "\n",
    "\n",
    "#function to find backward probabilities\n",
    "def backward_probs():\n",
    "    # node values stored during forward algorithm\n",
    "    node_values_bwd = np.zeros((len(states), len(test_sequence)))\n",
    "\n",
    "    #for i, sequence_val in enumerate(test_sequence):\n",
    "    for i in range(1,len(test_sequence)+1):\n",
    "        for j in range(len(states)):\n",
    "            # if first sequence value then do this\n",
    "            if (-i == -1):\n",
    "                node_values_bwd[j, -i] = end_probs[j]\n",
    "            # else perform this\n",
    "            else:\n",
    "                values = [node_values_bwd[k, -i+1] * emission[k, sequence_syms[test_sequence[-i+1]]] * transition[j, k] for k in range(len(states))]\n",
    "                node_values_bwd[j, -i] = sum(values)\n",
    "\n",
    "    #start state value\n",
    "    start_state = [node_values_bwd[m,0] * emission[m, sequence_syms[test_sequence[0]]] for m in range(len(states))]\n",
    "    start_state = np.multiply(start_state, start_probs)\n",
    "    start_state_val = sum(start_state)\n",
    "    return node_values_bwd, start_state_val\n",
    "\n",
    "\n",
    "#function to find si probabilities\n",
    "def si_probs(forward, backward, forward_val):\n",
    "\n",
    "    si_probabilities = np.zeros((len(states), len(test_sequence)-1, len(states)))\n",
    "\n",
    "    for i in range(len(test_sequence)-1):\n",
    "        for j in range(len(states)):\n",
    "            for k in range(len(states)):\n",
    "                si_probabilities[j,i,k] = ( forward[j,i] * backward[k,i+1] * transition[j,k] * emission[k,sequence_syms[test_sequence[i+1]]] ) \\\n",
    "                                                    / forward_val\n",
    "    return si_probabilities\n",
    "\n",
    "#function to find gamma probabilities\n",
    "def gamma_probs(forward, backward, forward_val):\n",
    "\n",
    "    gamma_probabilities = np.zeros((len(states), len(test_sequence)))\n",
    "\n",
    "    for i in range(len(test_sequence)):\n",
    "        for j in range(len(states)):\n",
    "            #gamma_probabilities[j,i] = ( forward[j,i] * backward[j,i] * emission[j,sequence_syms[test_sequence[i]]] ) / forward_val\n",
    "            gamma_probabilities[j, i] = (forward[j, i] * backward[j, i]) / forward_val\n",
    "\n",
    "    return gamma_probabilities\n",
    "\n",
    "\n",
    "\n",
    "#performing iterations until convergence\n",
    "\n",
    "for iteration in range(2000):\n",
    "\n",
    "    print('\\nIteration No: ', iteration + 1)\n",
    "    # print('\\nTransition:\\n ', transition)\n",
    "    # print('\\nEmission: \\n', emission)\n",
    "\n",
    "    #Calling probability functions to calculate all probabilities\n",
    "    fwd_probs, fwd_val = forward_probs()\n",
    "    bwd_probs, bwd_val = backward_probs()\n",
    "    si_probabilities = si_probs(fwd_probs, bwd_probs, fwd_val)\n",
    "    gamma_probabilities = gamma_probs(fwd_probs, bwd_probs, fwd_val)\n",
    "\n",
    "    # print('Forward Probs:')\n",
    "    # print(np.matrix(fwd_probs))\n",
    "    #\n",
    "    # print('Backward Probs:')\n",
    "    # print(np.matrix(bwd_probs))\n",
    "    #\n",
    "    # print('Si Probs:')\n",
    "    # print(si_probabilities)\n",
    "    #\n",
    "    # print('Gamma Probs:')\n",
    "    # print(np.matrix(gamma_probabilities))\n",
    "\n",
    "    #caclculating 'a' and 'b' matrices\n",
    "    a = np.zeros((len(states), len(states)))\n",
    "    b = np.zeros((len(states), len(sequence_syms)))\n",
    "\n",
    "    #'a' matrix\n",
    "    for j in range(len(states)):\n",
    "        for i in range(len(states)):\n",
    "            for t in range(len(test_sequence)-1):\n",
    "                a[j,i] = a[j,i] + si_probabilities[j,t,i]\n",
    "\n",
    "            denomenator_a = [si_probabilities[j, t_x, i_x] for t_x in range(len(test_sequence) - 1) for i_x in range(len(states))]\n",
    "            denomenator_a = sum(denomenator_a)\n",
    "\n",
    "            if (denomenator_a == 0):\n",
    "                a[j,i] = 0\n",
    "            else:\n",
    "                a[j,i] = a[j,i]/denomenator_a\n",
    "\n",
    "    #'b' matrix\n",
    "    for j in range(len(states)): #states\n",
    "        for i in range(len(sequence)): #seq\n",
    "            indices = [idx for idx, val in enumerate(test_sequence) if val == sequence[i]]\n",
    "            numerator_b = sum( gamma_probabilities[j,indices] )\n",
    "            denomenator_b = sum( gamma_probabilities[j,:] )\n",
    "\n",
    "            if (denomenator_b == 0):\n",
    "                b[j,i] = 0\n",
    "            else:\n",
    "                b[j, i] = numerator_b / denomenator_b\n",
    "\n",
    "\n",
    "    print('\\nMatrix a:\\n')\n",
    "    print(np.matrix(a.round(decimals=4)))\n",
    "    print('\\nMatrix b:\\n')\n",
    "    print(np.matrix(b.round(decimals=4)))\n",
    "\n",
    "    transition = a\n",
    "    emission = b\n",
    "\n",
    "    new_fwd_temp, new_fwd_temp_val = forward_probs()\n",
    "    print('New forward probability: ', new_fwd_temp_val)\n",
    "    diff =  np.abs(fwd_val - new_fwd_temp_val)\n",
    "    print('Difference in forward probability: ', diff)\n",
    "\n",
    "    if (diff < 0.0000001):\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
